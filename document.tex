%%This is a very basic article template.
%%There is just one section and two subsections.

\documentclass[accentcolor=tud9c,9.5pt,nochapname,bigchapter,paper=a5report]{tudreport}


\usepackage{mathtools}
\usepackage[pdftex,bookmarks=true]{hyperref}
\usepackage{ngerman}
\usepackage{lscape}
\usepackage{hyperref}
\usepackage{graphicx}
\DeclareMathSizes{9.5}{9.5}{7}{7}
\DeclareMathSizes{10}{10}{7}{7}
\DeclareMathSizes{11}{11}{8}{8}
\begin{document}

\def\Var{{\rm Var}\,}
\def\E{{\rm E}\,}
\def\freq{{(e^{j\omega})}\,}


\title{Stochastische Signale und Systeme}
\subtitle{Zusammenfassung Formeln}
\subsubtitle{Autor: Daniel Thiem - studium@daniel-thiem.de\\Version 0.9.3 - 24.09.2012}

\maketitle
\newpage
\thispagestyle{plain}
\mbox{}
\tableofcontents





\numberwithin{equation}{chapter}
\section*{Vorwort}
Fehler und Verbesserungen bitte an studium@daniel-thiem.de senden oder als Issue bei \url{https://github.com/Tyde/stosigsysfs/issues} melden. 
Der Quelltext dieser Formelsammlung ist auf \url{https://github.com/Tyde/stosigsysfs} und darf gerne erweitert werden.
\chapter{Kombinatorik \& reine Stochastik}

\section{Wahrscheinlichkeitsdichtefunktion}
Sei $F_X(x)$ die Verteilungsfunktion der Zufallsvariablen $X$
\begin{equation}
	f(x) = \frac{dF_X(x)}{dx}
\end{equation}

\subsection{Eigenschaften der Wahrscheinlichkeitsdichtefunktion}
\begin{subequations}
\begin{align}
	f_X(x)&\geq 0 \\
	f_X(x) &= P(X=x)
\end{align}
\end{subequations}

\subsection{Berechnung bei Abhängigkeit zu anderer Zufallsvariablen}
Sei $Y=g(X)$ und die Wahrscheinlichkeitsdichtefunktion von Y, $f_y(t)$, sei gesucht, während die Wahrscheinlichkeitsdichtefunktion $f_x(t)$ gegeben ist,
\begin{equation}
f_y(t)=f_x(g^{-1}(t))\left|\frac{d}{dy}g^{-1}\right|
\end{equation}

\section{Verteilungsfunktion}
$f(t)$ sei die Wahrscheinlichkeitsdichtefunktion der Zufallsvariablen $X$
\begin{equation}
	F(x) = P(X\leq x) = \int\limits_{-\infty}^x f(t)dt 
\end{equation}
\subsection{Eigenschaften der Verteilungsfunktion}
\begin{subequations}
\begin{align}
0 \leq F_X(x) &\leq 1 \\
F_X(\infty)&=1\\
F_X(-\infty)&=0 \\
F_X(x) \text{ist rechtsstetig, d.h. } \notag\\
\lim_{\epsilon \rightarrow 0} F_X(x+\epsilon) &= F_X(x)
\end{align}
\end{subequations}
\subsection{Wahrscheinlichkeitsrechnung mittels der Verteilungsfunktion}
\begin{subequations}
\begin{align}
F(a-) &= \lim_{\epsilon \rightarrow 0}  F_X(x-\epsilon)\\
P(X=a) &= F(a)-F(a-) \\
P(a<X\leq b) &=F(b)-F(a)\\
P(a\leq X<b) &=F(b-)-F(a-)\\
P(a\leq X\leq b) &=F(b)-F(a-)\\
P(X>a)&=1-F(a)
\end{align}
\end{subequations}
\section{Verteilungen}


\subsection{Normalverteilung}
\begin{equation}
f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{1}{2} \left(\frac{t-\mu}{\sigma}\right)^2}
\end{equation}

\subsection{Rechteckverteilung}
\begin{align}
f(t)&=\begin{cases}
\frac{1}{b-a} & a<t<b \\
0 & \text{sonst}
\end{cases} \\
F(x) &= \begin{cases}
0 & x \leq a \\
\frac{x-a}{b-a} & x \in (a,b] \\
1 & x > b
\end{cases}
\end{align}

\subsection{Exponentialverteilung}
\begin{align}
f(t)&=\begin{cases}
0 & t<0 \\
\lambda e^{-\lambda t} & t\geq 0
\end{cases} \\
F(x) &= \begin{cases}
0 & x < 0 \\
1-e^{-\lambda t} & x\geq 0
\end{cases}
\end{align}

\section{Formel von Bayes}

\begin{equation}
	P(A|B) = \frac{P(A \cap B)}{P(B)} 
	\Rightarrow P(A_k|B) = \frac{P(A_k \cdot P(B|A_k))}{\sum\limits_{i=1}^n P(B|A_i) \cdot P(A_i)}
\end{equation}



\section{Erwartungswerte}
\subsection{Erwartungswertberechnung}
\subsubsection{Allgemein}

Sei $f(x)$ die Wahrscheinlichkeitsdichtefunktion von $X$

\begin{equation}
	\E(X) = \int\limits_{-\infty}^\infty x \cdot f(x) dx
\end{equation}

\subsubsection{Erweitert}

Sei $Y=g(X)$ und $f(x)$ die Wahrscheinlichkeitsdichtefunktion von $X$

\begin{equation}
	\E[Y] = \E[g(X)] =  \int\limits_{-\infty}^\infty g(x) \cdot f(x) dx
\end{equation}

\subsection{Rechenregeln für Erwartungswerte}
Sei $A$ eine von $B$ unabhängige Zufallsvariable
\begin{equation}
	\E[A \cdot B] = \E[A] \cdot \E[B]
\end{equation}
Sei $X$ eine Zufallsvariable und $a,b$ jeweils Konstanten
\begin{equation}
	\E[aX +b] = a\E[X] + b
\end{equation}
Seien $X_i$ Zufallsvariablen
\begin{equation}
\E\left[\sum\limits_{i=0}^n X_i\right]=\sum\limits_{i=0}^n  \E[X_i]
\end{equation}
\section{Varianz}



\subsection{Berechnung der Varianz}
\begin{equation}
\Var(X)=\E(X^2)-\E(X)^2
\end{equation}
\subsection{Rechenregeln für Varianzen}
\begin{equation}
\Var(aX+b)=a^2 \Var(x)
\end{equation}
Seien $X_i$ Zufallsvariablen
\begin{equation}
\Var\left[\sum\limits_{i=0}^n X_i\right]=\sum\limits_{i=0}^n  \Var[X_i]
\end{equation}



\section{Konvergenz}
Es wird eine Konvergenz von Zufallsvariablen $X_k$ mit $k=0,1,2 \dots$ betrachtet:
\subsection{Konvergenz mit Wahrscheinlichkeit eins (Convergence with probability one)} \label{conv:one}
\begin{equation}
P \left(\lim_{k\rightarrow\infty} |X_k-X|=0\right)=1
\end{equation}

\subsection{Konvergenz im ``Mean Square Sense''} \label{conv:mss}
\begin{equation}
\lim_{k\rightarrow\infty} \E\left[|X_k-X|^2\right]=0
\end{equation}

\subsection{Convergence in Pobability} \label{conv:prob}
\begin{equation}
\lim_{k\rightarrow\infty}P \left( |X_k-X|>\epsilon\right)=0
\end{equation}

\subsection{Convergence in Distribution} \label{conv:dist}
\begin{equation}
\lim_{k\rightarrow\infty}F_{X_k} (x)=F_X(x) \quad \text{Für alle stetigen punkte $x$ aus } F_X
\end{equation}

\subsection{Gewichtung der Konvergenzen}
\begin{itemize}
  \item Convergence with probability 1 (\ref{conv:one}) implies convergence in probability (\ref{conv:prob})
  \item Convergence with probability 1 (\ref{conv:one}) implies convergence in the MSS (\ref{conv:mss}), provided second order moments exist.
  \item Convergence in the MSS (\ref{conv:mss}) implies convergence in probability (\ref{conv:prob}).
  \item Convergence in probability (\ref{conv:prob}) implies convergence in distribution (\ref{conv:dist}).
\end{itemize}
\chapter{Discrete-Time-Fourier-Transformation}

\section{Abtastung}
\subsection{Im Zeitbereich}
Sei $x_c(t)$ das zu abtastende Signal und $T_s=\frac{1}{f_s}$ die Abtastdauer bzw. Abtastfrequenz
\begin{equation}
x_s(t)=\sum\limits_{n=-\infty}^\infty x_c(nT_s)\delta(t-nT_s)
\end{equation}
\subsection{Im Frequenzbereich}

\begin{align}
X_s(j\Omega)&=\frac{1}{T_s}\sum\limits_{k=-\infty}^\infty X_c(j(\Omega-\frac{2\pi k}{T_s})) \notag\\
	&=\frac{1}{T_s}\sum\limits_{k=-\infty}^\infty X_c(j\Omega-kj\Omega_s) \quad \text{mit} \quad \Omega_s=\frac{2\pi}{T_s}\\
\end{align}
\section{Transformation}
\subsection{Rücktransformation}
\begin{align}
x[n]&=\int\limits_{-\pi}^\pi X\freq e^{j\omega n}d\omega 
\end{align}
\subsection{Zusammenhang $\Omega$ und $n$} \label{omegaToDescrete}
ACHTUNG: Dieser zusammenhang ist in SSS etwas anders im gegensatz zu dem Hilfsblatt von DSS
\begin{equation}
\omega = \Omega T_s
\end{equation}

\subsection{Dirac-Kamm}
\begin{equation}
\eta (\omega) = \sum\limits_{l=-\infty}^\infty \delta(\omega +2\pi l)
\end{equation}

\subsection{Berechnen einer Übertragungsfunktion im zeitdiskreten Fall}
\begin{enumerate}
	\item Zeitkontinuierliches $H\freq=\frac{Y\freq}{X\freq}$ berechnen
	\item Formel aus (\ref{omegaToDescrete}) einsetzen, um $H(j\Omega)$ zu erreichen
\end{enumerate}
\chapter{Prozesse}

\section{Strikte Stationarität} \label{stationarystrict}
\begin{equation}
F_x(x_1,\dots ,x_N;n_1,\dots,n_N) = F_x(x_1,\dots ,x_N;n_1+n_0,\dots ,n_N+n_0) \quad \text{mit $N\rightarrow \infty$}
\end{equation}

\section{Second order moment function(SOMF)} \label{somf}
\begin{equation}
r_{XX}(n_1,n_2)=\E[X(n_1)X(n_2)]
\end{equation}
\subsection{Stationär im weiteren Sinne} \label{stationary}
\begin{subequations}
\begin{align}
\E[X(n)]&=\text{const.} \\
r_{XX}(n_1,n_2) &= r_{XX}(\kappa) = \E[X(n+\kappa)\cdot X(n)] \quad \text{mit} \quad \kappa = |n_2-n_1|
\end{align}
\end{subequations}
\subsection{Eigenschaften der SOMF}
\begin{subequations}
\begin{align}
r_{XX}(0) &= \E[X(n)^2]=\sigma_X^2+\mu_x^2 \\
r_{XX}(\kappa) &= r_{XX}(-\kappa) \\
r_{XX}(0) &\geq|r_{XX}(\kappa)| \quad ,|\kappa|>0
\end{align}
\end{subequations}

\section{Cross-SOMF}
\begin{equation}
	r_{XY}(n_1,n_2) = \E[X(n_1) \cdot Y(n_2)]
\end{equation}

\subsection{Gemeinsame Statonarität (joint stationary)}\label{jointstationary}
Sei $X(n)$ und $Y(n)$ nach (\ref{stationary}) \emph{stationär}, dann sind die Prozesse gemeinsam stationär, wenn gilt:
\begin{equation} 
	r_{XY} = r_{XY}(n_1-n_2) = r_{XY}(\kappa) \quad\text{mit}\quad \kappa=n_1-n_2
\end{equation}
\subsection{Eigenschaften der Cross-SOMF}
\begin{subequations}
\begin{align}
r_{XY}(-\kappa) &= r_{YX}(\kappa) \\
|r_{XY}(\kappa)| &\leq \sqrt{r_{XX}(0) \cdot r_{YY}(0)} \\
|r_{XY}(\kappa)| &\leq \frac{1}{2}(r_{XX}(0)+r_{YY}(0))
\end{align}
\end{subequations}
\subsection{Unkorreliertheit (uncorrelated) anhand der Cross-SOMF}
\begin{equation}
	r_{XY}(\kappa)=\mu_x \cdot \mu_y = \E[X(n+\kappa)]\E[Y(n)]
\end{equation}
\subsection{Orthogonalität} \label{ortho}
\begin{equation}
	r_{XY}(\kappa)=0
\end{equation}
\section{Kovarianz (Covariance,Central-SOMF)} \label{covariance}
\begin{subequations}
\begin{align}
c_{XX}(n+\kappa,n) &= \E[(X(n+\kappa)-\E[X(n+\kappa)]) \cdot (X(n)-\E[X(n)])] \\
c_{XX}(n+\kappa,n) &= r_{XX}(n+\kappa,n)-\E[X(n+k)]\E[X(n)]
\end{align}
\end{subequations}
\subsection{Eigenschaften der Kovarianz}
Falls $X$ zumindest \emph{stationär im weiteren Sinne}(\ref{stationary}) ist, gilt
\begin{equation}
c_{XX}(\kappa)=r_{XX}(\kappa)-(\E[X(n)])^2 
\end{equation}

\subsection{Kovarianz einer zusammengesetzten Funktion}
\begin{subequations}
Falls $Y(n)=X(n)+V(n)$ und $X(n)$ ist von $V(n)$ statistisch unabhängig und einer der beiden Prozesse mittelwertfrei, dann gilt:
\begin{equation}
c_{YY}(\kappa)=C_{XX}(\kappa)+C_{VV}(\kappa)
\end{equation}
Ist $X(n)$ jedoch abhängig von $V(n)$, so gilt:
\begin{equation}
c_{YY}(\kappa)=C_{XX}(\kappa)+C_{VV}(\kappa)+C_{XV}(\kappa)+C_{VX}(\kappa)
\end{equation}

\end{subequations}

\subsection{Überführung der Central-SOMF in die Varianz}
\begin{equation}
c_{XX}(0)=\Var(X)
\end{equation}
\section{Kreuz-Kovarianz (Cross-covariance)} \label{crosscovariance}
\begin{subequations}
\begin{align}
c_{XY}(n+\kappa,n) &= \E[(X(n+\kappa)-\E[X(n+\kappa)]) \cdot (Y(n)-\E[Y(n)])] \\
c_{XY}(n+\kappa,n) &= r_{XY}(n+\kappa,n)-\E[X(n+k)]\E[Y(n)]
\end{align}
\end{subequations}

\subsection{Eigenschaften der Kreuzkovarianz}

Falls $X$ und $Y$ zumindest \emph{gemeinsam stationär im weiteren Sinne }(\ref{jointstationary}) sind, gilt:

\begin{equation}
c_{XY}(\kappa)=r_{XY}(\kappa)-\E[X(n)]\E[Y(n)]
\end{equation}


\subsection{Unkorreliertheit (uncorrelated) anhand der Kreuzkovarianz}\label{uncorrelated}
\begin{equation}
	c_{XY}(\kappa)=0
\end{equation}

\section{Komplexe Prozesse}
Seien $X(n)$ und $Y(n)$ reale Zufallsprozesse, so ist
\begin{equation}
Z(n)\hat{=}  X(n)+jY(n)
\end{equation}
ein Komplexer Zufallsprozess
\subsection{Erwartungswert eines Komplexen Zufallsprozess}
\begin{equation}
\E[Z(n)]=\E[X(n)]+j\E[Y(n)]
\end{equation}
\subsection{SOMF eines Komplexen Zufallsprozess}
\begin{equation}
r_{ZZ}(n_1,n_2)=\E[Z(n_1) \cdot Z(n_2)^*]
\end{equation}
\subsubsection{Besondere Eigenschaften}
Für einen komplexen Zufallsprozess, welcher \emph{stationär im weiteren Sinne}(\ref{stationary}) ist, gilt
\begin{equation}
r_{ZZ}(-\kappa)=r_{ZZ}(\kappa)^*
\end{equation}
\subsection{cross-SOMF komplexer Zufallsprozesse}

\begin{equation}
	r_{Z_1Z_2}(n_1,n_2) = \E[Z_1(n_1) \cdot Z_2(n_2)^*]
\end{equation}

\subsection{Kovarianz (Covariance) eines komplexen Zufallsprozess}
\begin{equation}
c_{ZZ}(n+\kappa,n) = \E[(Z(n+\kappa)-\E[Z(n+\kappa)]) \cdot (Z(n)-\E[Z(n)])^*]
\end{equation}

\subsection{Kreuzkovarianz(cross-covariance) komplexer Zufallsprozesse}
\begin{equation}
c_{Z_1Z_2}(n+\kappa,n) = \E[(Z_1(n+\kappa)-\E[Z_1(n+\kappa)]) \cdot (Z_2(n)-\E[Z_2(n)])^*] 
\end{equation}

\subsection{Eigenschaften komplexer Zufallsprozesse}
\emph{Unkorreliertheit} verhält sich wie (\ref{uncorrelated}), genauso wie \emph{Orthogonalität} (\ref{ortho})
\chapter{Spektraldichten (Power Spectral Density)}
% 
% 
% \section{Normierte Energie in einem Zeitintervall}
% \begin{subequations}
% \begin{align}
% E_N&=\sum\limits_{n=-M}^M x_N(n)^2 \\
% &=\int\limits_{-\pi}^\pi \left|X_N\left(e^{j\omega}\right)\right|^2 \frac{d\omega}{2\pi}
% \end{align}
% \end{subequations}
% \section{Durchschnittliche Leistung in einem Zeitintervall}
% \begin{subequations}
% \begin{align}
% P_N&=\frac{1}{2M+1}\sum\limits_{n=-M}^M x_N(n)^2 \\
% &=\int\limits_{-\pi}^\pi \frac{\left|X_N\left(e^{j\omega}\right)\right|^2}{2M+1} \frac{d\omega}{2\pi}
% \end{align}
% \end{subequations}

\section{Leistungsdichte}
\subsection{Leistungsspektraldichte (Power Spectral Density,PSD)} \label{psd}
\begin{align}
S_{XX}(e^{j\omega},\xi) &= \lim_{M \rightarrow \infty} \frac{\E\left[\left|X_N\left(e^{j\omega},\xi\right)\right|^2\right]}{2M+1}\\
\text{mit}\notag\\
X_N(e^{j\omega},\xi) &=\sum\limits_{n=-M}^M x_N(n,\xi) e^{-j\omega n}
\end{align}
\subsubsection{Eigenschaften der Leistungsspektraldichte}
\begin{subequations}
\begin{alignat}{3}
S_{XX}(e^{j \omega})^*&=S_{XX}(e^{j \omega}) \quad &\text{mit}\quad X(n)\in \mathbb{C} \\
S_{XX}(e^{j \omega})&\geq 0 \quad &\text{mit}\quad X(n)\in \mathbb{C} \\
S_{XX}(e^{-j \omega})&=S_{XX}(e^{j \omega}) \quad &\text{mit}\quad X(n)\in \mathbb{R} 
\end{alignat}
\end{subequations}
\subsection{Durchschnittliche Leistung eines Zufallsprozesses}
\begin{subequations}
\begin{align}
P_{XX}&=\int\limits_{-\pi}^\pi S_{XX}\freq \frac{d\omega}{2\pi} = r_{XX} (0)\\
&=\lim_{M \rightarrow \infty}\int\limits_{-\pi}^\pi \frac{\E\left[\left|X_N\left(e^{j\omega},\xi\right)\right|^2\right]}{2M+1} \frac{d\omega}{2\pi}
\end{align}
\end{subequations}
\subsection{Kreuzleistungsdichte (cross-power density)}
\begin{equation}
S_{XY}(e^{j\omega},\xi) = \lim_{M \rightarrow \infty} \frac{\E\left[X_N\left(e^{j\omega},\xi\right)Y_N\left(e^{j\omega},\xi\right)^*\right]}{2M+1}\\
\end{equation}
\subsubsection{Eigenschaften der Kreuzleistungsdichte}
\begin{subequations}
\begin{alignat}{3}
S_{XY}(e^{j \omega})^*&=S_{YX}(e^{j \omega}) \quad &\text{mit}\quad X(n),Y(n)\in \mathbb{C} \\
S_{XY}(e^{j \omega})^*&=S_{YX}(-e^{j \omega}) \quad &\text{mit}\quad X(n),Y(n)\in \mathbb{R} \\
\mathfrak{Re}\{S_{XY}(e^{j \omega})\}&\text{ und }\mathfrak{Re}\{S_{YX}(e^{j \omega})\} &\text{sind gerade, wenn } X(n),Y(n) \in \mathbb{R}\\
\mathfrak{Im}\{S_{XY}(e^{j \omega})\}&\text{ und }\mathfrak{Im}\{S_{YX}(e^{j \omega})\} &\text{sind ungerade, wenn } X(n),Y(n) \in \mathbb{R}\\
S_{XY}(e^{j \omega})&=S_{YX}(e^{j \omega}) =0 \quad &\text{wenn $X(n)$ und $Y(n)$ orthogonal (\ref{ortho})} 
\end{alignat}
\end{subequations}
\subsection{Durchschnittliche Kreuzleistung zweier Zufallsprozesse}
\begin{equation}
P_{XY}=\int\limits_{-\pi}^\pi S_{XY}\freq \frac{d\omega}{2\pi}
\end{equation}

\subsection{Wiener-Khinchine theorem}
Ist $X(n)$ ein \emph{im weiteren Sinne stationärer}(\ref{stationary}) Zufallsprozess, do kann die \emph{Leistungsspektraldichte} (\ref{psd}) 
aus der Fourier-Transformation der \emph{Momentenfunktion zweiter Ordnung(SOMF)} (\ref{somf}) gewonnen werden:
\begin{subequations}
\begin{align}
S_{XX}\freq&=\mathcal{F}\{r_{XX}(\kappa)\}=\sum\limits_{k=-\infty}^\infty r_{XX}(\kappa) e^{-k\omega \kappa} \\
&\quad\text{und invers} \notag\\
r_{XX}(\kappa)&=\mathcal{F}^{-1}\{S_{XX}\freq\}=\int\limits_{-\pi}^\pi S_{XY}(e^{j\omega \kappa})\frac{d\omega}{2\pi}
\end{align}
\end{subequations}

\subsection{Kreuzleistungsdichte durch Cross-SOMF}
\begin{equation}
S_{XY}\freq=\mathcal{F}\{r_{XY}(\kappa)\}=\sum\limits_{k=-\infty}^\infty r_{XY}(\kappa) e^{-k\omega \kappa}
\end{equation}


\section{Kohärenz (coherence)} \label{coherence}
\begin{equation}
	\rm{Coh}_{XY}\freq=\frac{\left|S_{XY}\freq\right|^2}{S_{XX}\freq S_{YY}\freq}
\end{equation}
\subsection{Eigenschaften der Kohärenz}
Die Kohärenz zwischen den Zufallsprozessen $X(n)$ und $Y(n)$ besagt, wie gut $X$ zu $Y$ bei einer gegebenen Frequenz $\omega$ korrespondiert.
\begin{equation}
0\leq\rm{Coh}_{XY}\freq\leq 1
\end{equation}

\section{Root Mean Square (RMS) und Gleichsstrom (DC) Werte}
\subsection{DC-Values}
\begin{equation}
X_{dc} = \lim_{M \rightarrow \infty} \frac{1}{2M+1} \sum\limits_{n=-M}^M X(n)=\E[X(n)]=\mu_X
\end{equation}
\subsection{Normalisierte DC-Leistung}
\begin{equation}
P_{dc} = \left[\lim_{M \rightarrow \infty} \frac{1}{2M+1} \sum\limits_{n=-M}^M X(n)\right]^2 = \E[X(n)]^2 =X_{dc}^2
\end{equation}
\subsection{RMS-Value}
\begin{equation}
X_{RMS}=\sqrt{\lim_{M \rightarrow \infty} \frac{1}{2M+1} \sum\limits_{n=-M}^M X(n)^2}=\sqrt{r_{XX}(0)}=\sqrt{\int\limits_{-\pi}^\pi S_{XX}\freq \frac{d\omega}{2\pi} }
\end{equation}


%Evtl noch EMS Value of AC PArt

\section{Spektrum}
\subsection{Spektrum eines stationären Zufallsprozesses}
Ist $X(n)$ ein \emph{stationärer} (\ref{stationarystrict}) Zufallsprozess, so ist sein 
Spektrum die Fouriertransformierte der \emph{Kovarianzfunktion} (\ref{covariance})
\begin{equation}
C_{XX}(e^{j \omega})=\sum\limits_{n=-\infty}^\infty c_{xx}(n)e^{-j\omega n}
\end{equation}


\subsubsection{Eigenschaften des Spektrums}
\begin{enumerate}
  \item Wenn $\sum_n |c_{XX}(n)|<\infty$, dann existiert $C_{XX}$ und ist begrenzt und stetig
  \item $C_{XX}$ ist Real, $2\pi$-Periodisch und $C_{XX}\geq0$
  \item \begin{equation}
  c_{XX}(n)=\frac{1}{2\pi}\int_{-\pi}^{\pi}C_{XX}(e^{j \omega})e^{j \omega n}d\omega
  \end{equation}
\end{enumerate}

\subsection{Kreuzspektrum zweier gemeinsam stationärer Zufallsprozesse}
Ist $X(n)$ und $Y(n)$ \emph{gemeinsam stationär} (\ref{jointstationary}), dann ist das Kreuzspektrum definiert durch
\begin{equation}
C_{XY}(e^{j \omega})=\sum\limits_{n=-\infty}^\infty c_{XY}(n)e^{-j \omega n}
\end{equation}

\subsubsection{Eigenschaften der Kreuzspektrums}
Das Spektrum eines Realen Zufallsprozesses ist komplett im Intervall $[0,\pi]$ bestimmt
\begin{subequations}
\begin{align}
	C_{XY}(e^{j \omega})&=C_{YX}(e^{j \omega})^* \\
	c_{XY}(n) &= \frac{1}{2\pi}\int\limits_{-\pi}^\pi C_{XY}(e^{j \omega})e^{j \omega n} d\omega \\
	\text{Wenn } &X(n),Y(n) \in \mathbb{R} \text{ dann} \notag\\
	C_{XX}(e^{j \omega}) &= C_{XX}(e^{-j \omega})\\
	C_{XY}(e^{j \omega}) =C_{XY}(e^{-j \omega})^*&=C_{YX}(e^{-j \omega})=C_{YX}(e^{j \omega})^*
\end{align}
\end{subequations}


\chapter{Filter}
\section{Lineare Filter}
Wenn $X(n)$ und $Y(n)$ \emph{stationär} (\ref{stationarystrict}) sind, $h(n)$ eine Impulsantwort eines LTI-Systems ist
 und das Filter \emph{stabil} (\ref{Stability}) ist, existiert mit \emph{Wahrscheinlichkeit eins} (\ref{conv:one})
 das lineare Filter mit:
\begin{equation} \label{eq:linfil}
Y(n)=\sum\limits_{k=-\infty}^\infty h(k)X(n-k)=\sum\limits_{k=-\infty}^\infty h(n-k)X(k)
\end{equation}
\subsection{Stabilität} \label{Stability}
Die Stabilität eines Filters ist gegeben, wenn:
\begin{equation}
\sum|h(n)|<\infty
\end{equation}
\emph{Alternativ:} Sei $H(z)$ die z-Transformation des Filters $h(n)$. Dann ist das Filter
stabil, falls die Polstellen von $H(z)$  innerhalb des Einheitskreises liegen

\subsection{Eigenschaften eines Linearen Filters}
Die folgenden Eigenschaften gelten nur, wenn das Filter \emph{stabil} (\ref{Stability}) ist
\begin{itemize}
  \item Ist $X(n)$ \emph{stationär} (\ref{stationarystrict}) und $\E[|X(n)|]<\infty$, dann ist $Y(n)$ \emph{stationär}
  \item $Y(n)$ wird linearer Prozess genannt (linear process)
\end{itemize}
\subsection{Instabiler linearer Filter}
Ist das Filter nicht \emph{stabil} (\ref{Stability}), aber $\int|H(e^{j\omega}|d\omega<\infty)$ trifft zu 
und für $X(n) \quad \sum|c_{XX}(n)<\infty$, sodann existiert im \emph{Mean-Square-Sense} (\ref{conv:mss}) die Formel (\ref{eq:linfil})
und $Y(n)$ ist \emph{stationär im weiteren Sinne} (\ref{stationary}) mit
\begin{equation}
\mu_Y=\E[Y(n)]=\sum\limits_{k=-\infty}^\infty h(k)\E[X(n-k)]=\mu_XH(e^{j0})
\end{equation}

\subsection{Leistungsdichtespektrum des Ausgangs eines Filters}
Sei die Übertragungsfunktion des Filters $H\freq=\frac{Y\freq}{X\freq}$, und das Leistungsdichtspektrum von $X(n)$ sei $S_{XX}\freq$, dann gilt:
\begin{equation}
S_{YY}\freq=|H\freq|^2S_{XX}\freq
\end{equation}  

\subsection{Spektrum/Kovarianz des Ausgangs eines Filters}
Sei die Übertragungsfunktion des Filters $H\freq=\frac{Y\freq}{X\freq}$, und das Sepektrum von $X(n)$ sei $C_{XX}\freq$, dann gilt:
\begin{subequations}
\begin{align}
C_{YY}\freq&=|H\freq|^2C_{XX}\freq\\
c_{YY}(\kappa)&=\sum\limits_{k=-\infty}^\infty \sum\limits_{l=-\infty}^\infty h(k)h(l) \cdot c_{XX}(\kappa-k+1)
\end{align}
\end{subequations}

\subsection{Kreukovarianz des Ausgangs des Filters}
Sei $X(n)$ das Eingangssignal und $Y(n)$ das Ausgangssignal
\begin{subequations}
\begin{align}
c_{YX}&=\sum\limits_{k=-\infty}^\infty h(k)c_{XX}(\kappa -k) \\
C_{YX}\freq &=H\freq C_{XX}\freq\\
c_{YX}&=\int\limits_{-\pi}^\pi H\freq C_{XX}\freq e^{-j\omega\kappa}\frac{d\omega}{2\pi}
\end{align}
\end{subequations}

\subsection{Kreukovarianz des Ausgangs zweier paralleler Filter}

\begin{subequations}
\begin{align}
c_{Y_1Y_2}(\kappa)&=\sum\limits_{k=-\infty}^\infty \sum\limits_{l=-\infty}^\infty h_1(k)h_2(l)\cdot c_{X_1X_2}(\kappa -k+l) \\
c_{Y_1Y_2}(\kappa)&=h_1(\kappa)\star h_2(\kappa)^*\star c_{X_1X_2}(\kappa)\\
C_{Y_1Y_2}\freq &=H_1\freq H_2\freq^*C_{X_1X_2}\freq
\end{align}
\end{subequations}

\subsection{Kaskade linearer Filter}
\begin{subequations}
\begin{align}
H\freq&=\prod\limits_{i=1}^L H_i\freq \\
C_{YY}\freq&=C_{XX}\freq\prod\limits_{i=1}^L\left| H_i\freq\right|^2 \\
C_{YX}\freq&=C_{XX}\freq\prod\limits_{i=1}^L H_i\freq
\end{align}
\end{subequations}
\section{Matched Filter}

\subsection{Annahmen des Matched Filters}
\begin{itemize}
  \item Das eingehende Signal $X(n)$ besteht entweder aus einem Signal mit Rauschen oder nur Rauschen:\begin{equation}
  X(n)=\begin{cases}
  s(n)+V(n)\\
  V(n)
  \end{cases}
  \end{equation}
  \item Dabei ist $s(n)$ reelwertig, deterministisch und betrachtet in $n \in [0,N)$
  \item $E[V(n)]=0$ und $C_{VV}\freq$ bekannt
\end{itemize}

\subsection{Ziel des Matched Filters}
Maximierung des Signal-Rausch-Verhältnis:
\begin{equation}
\left(\frac{S}{N}\right)=\max\frac{|s_0(n_0)|^2}{\E[V_0(n_0)^2]}
\end{equation}

\subsection{Übertragungsfunktion des Matched Filters}
Sei $S\freq = \mathcal{F}\{s(n)\}$, $C_{VV}$ das Spektrum des Rauschens,
 $n_0$ die Abtastunszeit, bei welcher $(S/N)$ berechnet wird, und $k$ eine reele Konstante
\begin{equation}
H\freq = k \frac{S\freq^*}{C_{VV}\freq}e^{-j\omega n_0}
\end{equation}
Dabei geht der Signalverlauf am Ende des Filters verloren und der Filter kann zur Signaldetektion genutzt werden
\subsection{Matched Filter für Wei\ss es Rauschen}
Bei wei\ss em Rauschen wird die Impulsantwort des Filters zu
\begin{equation}
h(n) \equiv c \cdot s(n_0 - n)
\end{equation}
$\Rightarrow$ Die Impulsantwort des Filters ist das bekannte Signal ''rückwärts gespielt'' und um $n_0$ verschoben\\
Der Signal zu Rausch Abstand ergibt sich dann zu:
\begin{equation}
\left(\frac{S}{N}\right)_{out}=\frac{E_s}{\sigma_V^2}
\end{equation}


\section{Wiener Filter}
\subsection{Ziel des Wiener Filters}
Der Wiener Filter versucht die optimale Schätzung (nach (\ref{conv:mss})) eines Zufallsprozesses durch die Beobachtung eines anderen Prozesses
\subsection{Annahmen des Wiener Filters}
\includegraphics[width=\textwidth]{images/wienerfilter.JPG}
\begin{itemize}
  \item $X(n)$ ist der zu schätzende Zufallsprozess
  \item $Y(n)$ ist der betrachtete Zufallsprozess
  \item $\epsilon(n)$ ist der Fehlerprozess
  \item $X(n)$ und $Y(n)$ sind reelwertig, mittelwertfrei und \emph{gemeinsam stationär im weiteren Sinne} (\ref{jointstationary})
  \item Aufgrund der \emph{gemeinsamen Stationarität im weiteren Sinne} (\ref{jointstationary}) der beiden Prozesse ist die Impulsantwort
  		$h(n)$ stabil und der Fehlerprozess $\epsilon(n)$ \emph{stationär im weiteren Sinne} (\ref{stationary})
\end{itemize}


\subsection{Die Übertragungsfunktion des Wiener Filters}
Enstehend aus den \emph{Wiener-Hopf-Gleichungen}
\begin{subequations}
\begin{alignat}{2}
c_{XY}(\kappa)&=h_{opt}(\kappa)\star C_{YY}(\kappa) \quad &\kappa \in \mathbb{Z} \\
C_{XY}\freq &=G_{opt}\freq C_{YY}\freq \quad &\omega \in \mathbb{R}
\end{alignat}
\end{subequations}
erlangt man die optimale Übertragungsfunktion:
\begin{equation}
H_{opt}\freq = \frac{C_{XY}\freq}{C_{YY}\freq}
\end{equation}

\subsection{Mean Square Error des Wiener Filters}
\begin{subequations}
\begin{align}
q_{min}&=C_{XX}(0)-\sum\limits_{m=-\infty}^\infty h_{opt}(m)C_{XY}(m) \\
q_{min}&=p(0) \quad \text{mit} \\
p(\kappa) &= C_{XX}(\kappa)-h_{opt}(\kappa)\star c_{YX}(\kappa) \notag
\end{align}
\end{subequations}

\subsection{Der Wiener Filter mit additivem Rasuchen}
\begin{equation}
H_{opt}\freq = \frac{C_{XX}\freq}{C_{XX}\freq + C_{VV}\freq}
\end{equation}
\chapter{Sonstiges}
\section{Spezielle Funktionen}
\subsection{Gaussian white noise process} \label{whitenoise}
Gaußsches wei\ss es Rauschen ist immer \emph{stationär} (\ref{stationarystrict}) 
\begin{subequations}
\begin{align}
\E[W(n)]&=0 \\
r_{WW}(\kappa)=c_{WW}(\kappa)&=\sigma_W^2\delta(\kappa) \\
S_{WW}(e^{j \omega})&=\sigma_W^2
\end{align}
\end{subequations}

\subsection{Kronecker delta function}
\begin{equation}
\delta(\kappa)=\begin{cases}
1 &\kappa = 0\\
0 &\kappa \neq 0
\end{cases}
\end{equation}


\section{Mathematische nützliche Formeln}
\subsection{Ungleichung von Schwarz}
\begin{equation}
\left|\int\limits_a^b \varphi_1(\omega)\varphi_2(\omega)d\omega \right|^2\leq \left( \int\limits_a^b |\varphi_1(\omega)|^2 d\omega \right)\cdot\left( \int\limits_a^b |\varphi_2(\omega)|^2 d\omega \right)
\end{equation}


\subsection{Orthogonalitäts- und Normierungsbeziehungen}
\begin{subequations}
\begin{alignat}{3}
\int\limits_0^{2\pi} cos(mt)cos(nt) dt &= 0 \quad &\text{für } m \neq n \\
\int\limits_0^{2\pi} sin(mt)sin(nt) dt &= 0 \quad &\text{für } m \neq n \\
\int\limits_0^{2\pi} cos(mt)sin(nt) dt &= 0 \quad & \\
\int\limits_0^{2\pi} cos^2(nt) &= \begin{cases}
\pi &\text{für } n\geq1\\
2\pi &\text{für } n = 0
\end{cases} \\
\int\limits_0^{2\pi} sin^2(nt) &= \begin{cases}
\pi &\text{für } n\geq1\\
0 &\text{für } n = 0
\end{cases} \\
\int\limits_0^{2\pi} cos(k+nt) dt &= 0 \quad &\text{mit } k=const\\
\int\limits_0^{2\pi} sin(k+nt) dt &= 0 \quad &\text{mit } k=const
\end{alignat}
\end{subequations}

\subsection{Betragsquadrat komplexer Funktionen}
\begin{equation}
|H\freq|^2=H\freq H(e^{-j \omega})
\end{equation}



\end{document}